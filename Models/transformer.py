# -*- coding: utf-8 -*-
"""transformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yAoN-mXViwp76o-wtMPc_efQKmMnEMj7
"""

!pip install torchdata

from google.colab import drive
import os
drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/My\ Drive/NLP Spring 22/Project/

import torch
from torch import nn
from torch import optim
from torch import Tensor
from torchtext.data.utils import get_tokenizer
from torchtext.datasets import AG_NEWS, WikiText103
from torchtext.vocab import build_vocab_from_iterator
import torch.nn.functional as F
import math
from torch.autograd import Variable
from torch.nn.utils.rnn import pad_sequence

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

tokenizer = get_tokenizer('basic_english')
train_iter = WikiText103(split='train')

def yield_tokens(data_iter):
  for text in data_iter:
    yield tokenizer(text)

vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=["<unk>", "<pad>", "<cls>", "<SOS>", "<EOS>"])
vocab.set_default_index(vocab["<unk>"])

text_pipeline = lambda x: vocab(tokenizer(x))
label_pipeline = lambda x: vocab(tokenizer(x))

def sent2TensorOld(sent='my name is mashrur'):
  snt = text_pipeline(sent)
  snt = snt + vocab(["<EOS>"])

  return torch.tensor(snt, dtype=torch.long, device=device).view(-1, 1)

def ind2word(x):
  return vocab.lookup_token(x)

def sent2Tensor(x):
  return torch.tensor(text_pipeline(x), dtype=torch.int64).unsqueeze(0).view(-1, 1).to(device)

import csv
import math
import random

filename = 'data/Context_to_Option_Mixed.csv'
xx = []
yy = []
dats = []

with open(filename, 'r') as f:
  reader = csv.reader(f, delimiter=',')
  header = next(reader)

  for row in reader:
    genre = str(row[0]).strip()
    rep_genre = '<' + genre + '>'
    n_genre = ' < ' + genre + ' > '
    #n_genre = ''
    dats.append((
        row[1].replace('<', ' <').replace('>', '> ').replace('  ', ' ').replace(rep_genre, n_genre).strip(),
        row[2].replace('<', ' <').replace('>', '> ').replace('  ', ' ').strip()
    ))
    
random.shuffle(dats)

for ii in dats:
  xx.append(ii[0])
  yy.append(ii[1])

def create_split():
  train_x = []
  train_y = []
  test_x = []
  test_y = []
  val_x = []
  val_y = []

  ln = len(xx)
  tr_ln = int(.8*ln)
  tst_ln = int(0.15*ln)

  for i in range(tr_ln):
    train_x.append(xx[i])
    train_y.append(yy[i])

  for i in range(tr_ln, tr_ln+tst_ln):
    test_x.append(xx[i])
    test_y.append(yy[i])

  for i in range(tr_ln+tst_ln, ln):
    val_x.append(xx[i])
    val_y.append(yy[i])


  return train_x, train_y, test_x, test_y, val_x, val_y

class Dataset(torch.utils.data.Dataset):
  def __init__(self, inp, trg):
    self.input = [text_pipeline(x) for x in inp]
    self.target = [label_pipeline(x) for x in trg]
    self.input_original = inp
    self.target_original = trg

  def __getitem__(self, idx):
    data = {'X': torch.tensor(self.input[idx], dtype=torch.int64).to(device=device), 
            'Y': torch.tensor(self.target[idx], dtype=torch.int64).to(device=device),
            'Y_o': self.target_original[idx]}
    return data

class PositionalEncoding(nn.Module):
  def __init__(self, d_model, dropout=0.1, max_length=5000):
    super(PositionalEncoding, self).__init__()

    self.d_model = d_model
    self.max_length = max_length

    pos_embedding = torch.zeros((self.max_length, self.d_model))
    pos = torch.arange(0, self.max_length).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, self.d_model, 2) *
                             -math.log(10000.0) / self.d_model)
    
    pos_embedding[:, 0::2] = torch.sin(pos * div_term)
    pos_embedding[:, 1::2] = torch.cos(pos * div_term)
    pos_embedding = pos_embedding.unsqueeze(-2)
    self.register_buffer('pos_embedding', pos_embedding)
    self.dropout = nn.Dropout(dropout)

  def forward(self, token_embedding: Tensor):
    x = token_embedding + self.pos_embedding[:token_embedding.size(0), :]
    return self.dropout(x)

class Transformer(nn.Module):
  def __init__(self, en_layer, de_layer, d_model, nhead, vocab_size, 
               dim_ffn=512, dropout=0.1):
    super(Transformer, self).__init__()
    self.transformer = nn.Transformer(
        d_model=d_model,
        nhead=nhead,
        num_encoder_layers=en_layer,
        num_decoder_layers=de_layer,
        dim_feedforward=dim_ffn,
        dropout=dropout
    )
    self.d_model = d_model
    self.generator = nn.Linear(d_model, vocab_size)
    self.src_token_embedding = nn.Embedding(vocab_size, d_model)
    self.tgt_token_embedding = nn.Embedding(vocab_size, d_model)
    self.pos_enc = PositionalEncoding(d_model, dropout)

  def forward(self, src, tgt, src_mask, tgt_mask, src_pad_mask, 
              tgt_pad_mask, mem_key_pad_mask):
    src_emb = self.pos_enc(self.src_token_embedding(src.long())*
                           math.sqrt(self.d_model))
    
    tgt_emb = self.pos_enc(self.tgt_token_embedding(tgt.long())*
                           math.sqrt(self.d_model))
    out = self.transformer(src=src_emb, 
                           tgt=tgt_emb, 
                           src_mask=src_mask, 
                           tgt_mask=tgt_mask, 
                           src_key_padding_mask=src_pad_mask, 
                           tgt_key_padding_mask=tgt_pad_mask, 
                           memory_key_padding_mask=mem_key_pad_mask)
    
    return self.generator(out)

  def encode(self, src, src_mask):
    src_emb = self.pos_enc(self.src_token_embedding(src.long())*
                           math.sqrt(self.d_model))
    return self.transformer.encoder(src_emb, src_mask)

  def decode(self, tgt, mem, tgt_mask):
    tgt_emb = self.pos_enc(self.tgt_token_embedding(tgt.long())*
                           math.sqrt(self.d_model))
    return self.transformer.decoder(tgt_emb, mem, tgt_mask)

MAX_LENGTH = 5000
SOS = vocab(["<SOS>"])[0]
EOS = vocab(["<EOS>"])[0]
CLS = vocab(["<cls>"])[0]
PAD = vocab(["<pad>"])[0]

# Helpers
from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu

def generate_sq_sub_mask(sz):
  mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)
  mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
  return mask

def get_mask(src, tgt):
  src_len = src.shape[0]
  tgt_len = tgt.shape[0]

  tgt_mask = generate_sq_sub_mask(tgt_len)
  src_mask = torch.zeros((src_len, src_len), device=device).type(torch.bool)

  src_pad_mask = (src == PAD).transpose(0, 1)
  tgt_pad_mask = (tgt == PAD).transpose(0, 1)

  return src_mask, tgt_mask, src_pad_mask, tgt_pad_mask


def bleu(ref, gen):
  ref[0] = '<SOS> ' + ref[0] + ' <EOS>'
  ref_bleu = []
  gen_bleu = []
  for l in gen:
    gen_bleu.append(l.split())
  for i,l in enumerate(ref):
    ref_bleu.append([l.split()])
  cc = SmoothingFunction()
  score_bleu = corpus_bleu(ref_bleu, gen_bleu, weights=(0, 1, 0, 0), smoothing_function=cc.method4)
  return score_bleu

def pad_tensor(x):
  x = x.reshape(1, -1)
  x_ln = MAX_LENGTH - x.shape[1]
  
  x = F.pad(x, (0, x_ln), "constant", PAD)
  x = x.reshape(-1, 1)

  return x

def train_sgd(src, tgt, model, opt, criterion):
  #src = pad_sequence(src, padding_value=PAD)
  #tgt = pad_sequence(tgt, padding_value=PAD)
  #print(src.shape)
  #print(tgt.shape)
  
  tgt_input = tgt[:-1, :]
  src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = get_mask(src, tgt_input)

  logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, 
                 tgt_padding_mask, src_padding_mask)
  
  opt.zero_grad()
  tgt_out = tgt[1:, :]
  loss = criterion(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
  loss.backward()

  opt.step()

  return loss.item()

def val_instance(src, tgt, model, criterion):
  tgt_input = tgt[:-1, :]
  words = []
  src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = get_mask(src, tgt_input)

  logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, 
                 tgt_padding_mask, src_padding_mask)
  
  logits = logits.reshape(-1, logits.shape[-1])
  
  for i in range(logits.shape[0]):
    words.append(ind2word(torch.argmax(logits[i]).item()))

  tgt_out = tgt[1:, :]
  loss = criterion(logits, tgt_out.reshape(-1))

  return loss.item(), ' '.join(words)

def train(model, opt, criterion, train_data, lr=0.01):
  model.train()
  all_loss = []
  
  for data in train_data:
    loss = train_sgd(data['X'].unsqueeze(0).view(-1, 1), data['Y'].unsqueeze(0).view(-1, 1), model, opt, criterion)
    all_loss.append(loss)
  
  return all_loss

def validation(model, criterion, train_data, lr=0.01):
  model.eval()
  all_loss = []
  all_bleu = []
  
  for data in train_data:
    loss, sent = val_instance(data['X'].unsqueeze(0).view(-1, 1), data['Y'].unsqueeze(0).view(-1, 1), model, criterion)
    all_loss.append(loss)
    all_bleu.append(bleu([sent], [data['Y_o']]))
  
  return all_loss, all_bleu

def getSampleX():
  return ['<SOS> lol <cls> my name is mashrur <EOS>']
def getSampleY():
  return ['<SOS> lol1 <cls> lol2 <cls> lol3 <cls> lol4 <EOS>']

train_x, train_y, test_x, test_y, val_x, val_y = create_split()
train_data = Dataset(train_x, train_y)
test_data = Dataset(test_x, test_y)
val_data = Dataset(val_x, val_y)

from statistics import mean

torch.manual_seed(0)

en_layer = 3
de_layer = 3
d_model = 512
nhead = 8
vocab_size = vocab.__len__()
dim_ffn=512
dropout=0.1
learning_rate = 0.0001

def comp_train(epochs = [2, 3, 5, 7, 9, 11, 13, 15]):
  train_losses = []
  val_losses = []
  bleu_scores = []
  best_bleu_score = -10000
  best_model = None

  for ep in epochs:
    train_losses_ep = []
    val_losses_ep = []
    bleu_scores_ep = [] 
    transformer = Transformer(en_layer, de_layer, d_model, nhead, 
                          vocab_size, dim_ffn, dropout)

    for param in transformer.parameters():
      if param.dim() > 1:
        nn.init.xavier_uniform(param)

    transformer = transformer.to(device)

    criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD)
    opt = torch.optim.Adam(transformer.parameters(), lr=learning_rate, 
                             betas=(0.9, 0.98), eps=1e-9)   
    for it in range(ep):
      train_loss = train(transformer, opt, criterion, train_data, lr=learning_rate)
      train_loss_avg = mean(train_loss)

      val_loss, all_bleu = validation(transformer, criterion, test_data, lr=learning_rate) # here replace test_data with val_data iterator
      val_loss_avg = mean(val_loss)
      bleu_score_avg = mean(all_bleu)

      if bleu_score_avg > best_bleu_score:
        best_bleu_score = bleu_score_avg
        best_model = transformer

      train_losses_ep.append(train_loss_avg)
      val_losses_ep.append(val_loss_avg)
      bleu_scores_ep.append(bleu_score_avg)

    train_losses.append(mean(train_losses_ep))
    val_losses.append(mean(val_losses_ep))
    bleu_scores.append(mean(bleu_scores_ep))

    print('Epoch: {}, Train Loss: {}, Val Loss: {}, BLEU: {}'.format(ep, 
                                                                     train_losses[-1],
                                                                     val_losses[-1],
                                                                     bleu_scores[-1]))
  return train_losses, val_losses, bleu_scores, best_model

epochs = [1, 2, 3, 5, 7, 9]

train_losses, val_losses, bleu_scores, best_model = comp_train(epochs)

print(bleu_scores)

"""# Plot Data"""

import matplotlib.pyplot as plt
plt.switch_backend('agg')
import matplotlib.ticker as ticker
import numpy as np


def showPlot(points, epochs, filename, scorename):
    #plt.figure()
    #fig, ax = plt.subplots()
    # this locator puts ticks at regular intervals
    #loc = ticker.MultipleLocator(base=0.2)
    #ax.yaxis.set_major_locator(loc)
    plt.xlabel('Number of epochs')
    plt.ylabel(scorename)
    plt.plot(epochs, points)

    plt.savefig(filename)

bleu_scores = [x*100 for x in bleu_scores]

showPlot(bleu_scores, epochs, 'plots/transformerbleu.png', 'BLEU')
showPlot(val_losses, epochs, 'plots/transformerval.png', 'Test Loss')

"""# Saving Trained Model"""

# Commented out IPython magic to ensure Python compatibility.
# %ls

torch.save(best_model.state_dict(), 'models/transformer.pt')

load_transformer = Transformer(en_layer, de_layer, d_model, nhead, 
                          vocab_size, dim_ffn, dropout).to(device)

load_transformer.load_state_dict(torch.load('models/transformer.pt'))

"""# Code for Prediction"""

def greedy_decode(model, src, src_mask, max_len, start_symbol=SOS):
    src = src.to(device)
    src_mask = src_mask.to(device)

    memory = model.encode(src, src_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)
    for i in range(max_len-1):
        memory = memory.to(device)
        tgt_mask = (generate_sq_sub_mask(ys.size(0))
                    .type(torch.bool)).to(device)
        out = model.decode(ys, memory, tgt_mask)
        out = out.transpose(0, 1)
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.item()

        ys = torch.cat([ys,
                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
        if next_word == EOS:
            break
    return ys

def get_prediction(model, sentence, max_length = MAX_LENGTH):
  model.eval()
  inp = sent2Tensor(sentence).view(-1, 1)
  inp_ln = inp.shape[0]

  inp_mask = (torch.zeros(inp_ln, inp_ln)).type(torch.bool)
  tgt_tokens = greedy_decode(model, inp, inp_mask, max_len=inp_ln + 5, start_symbol=SOS).flatten()
  
  out = list(tgt_tokens.cpu().numpy())
  words = [ind2word(x) for x in out]

  return ' '.join(words).replace("<SOS>", " ").replace("<EOS>", " ")

inps = []
with open('inps.txt', 'r') as f:
  inps = f.readlines()
  inps = [x.replace('<', ' <').replace('>', '> ').replace('  ', ' ').strip() for x in inps]
  print(inps)

for x in inps:
  xx = get_prediction(best_model, sentence=x)
  print(xx)

