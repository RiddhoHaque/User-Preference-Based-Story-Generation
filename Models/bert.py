# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZMO9M8E8s720fh_qFx-fd2qc0_M261Vk
"""

#!pip install transformers
!pip install transformers[sentencepiece]
!pip install datasets

!pip install rouge_score

from google.colab import drive
import os
drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/My\ Drive/NLP Spring 22/Project/

# Commented out IPython magic to ensure Python compatibility.
# %ls

import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

from transformers import BertGenerationEncoder, BertGenerationDecoder, EncoderDecoderModel
from transformers import BertTokenizer, AutoTokenizer
import torch
from transformers import TrainingArguments, Seq2SeqTrainingArguments
from transformers import Trainer, Seq2SeqTrainer
from datasets import load_metric

bert2bert = EncoderDecoderModel.from_pretrained("google/roberta2roberta_L-24_discofuse").to(device)

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

tokenizer.bos_token = '<BOS>'
tokenizer.eos_token = '<EOS>'
tokenizer.cls_token = '<cls>'
tokenizer.pad_token = '<pad>'
tokenizer.unk_token = '<unk>'

class Dataset(torch.utils.data.Dataset):
  def __init__(self, inp, trg):
    self.input = inp
    self.target = trg

  def __getitem__(self, idx):
    item = {}
    item['input_ids'] = torch.tensor(self.input['input_ids'][idx])
    #item['attention_mask'] = self.input[idx]['attention_mask']
    item['labels'] = torch.tensor(self.target['input_ids'][idx])
    return item

  def __len__(self):
    return len(self.target['input_ids'])

import csv
import math
import random

filename = 'data/Context_to_Option_v3.csv'
xx = []
yy = []
dats = []

with open(filename, 'r') as f:
  reader = csv.reader(f, delimiter=',')
  header = next(reader)

  for row in reader:
    dats.append((
        row[1].replace('<', ' <').replace('  ', ' ').strip(),
        row[2].replace('<', ' <').replace('  ', ' ').strip()
    ))
    
random.shuffle(dats)

for ii in dats:
  xx.append(ii[0])
  yy.append(ii[1])

def create_split():
  train_x = []
  train_y = []
  val_x = []
  val_y = []

  ln = len(xx)
  tr_ln = int(.8*ln)

  for i in range(tr_ln):
    train_x.append(xx[i])
    train_y.append(yy[i])

  for i in range(tr_ln, ln):
    val_x.append(xx[i])
    val_y.append(yy[i])

  return train_x, train_y, val_x, val_y

train_x, train_y, val_x, val_y = create_split()


train_x = tokenizer(train_x, truncation=True)
train_y = tokenizer(train_y, truncation=True)

val_x = tokenizer(val_x, truncation=True)
val_y = tokenizer(val_y, truncation=True)


train_data = Dataset(train_x, train_y)
val_data = Dataset(val_x, val_y)

training_args = Seq2SeqTrainingArguments("test_arguments")
training_args.predict_with_generate = True
training_args.num_train_epochs = 10
training_args.per_device_eval_batch_size=1
training_args.per_device_train_batch_size=1

trainer = Seq2SeqTrainer(
    model=bert2bert, 
    args=training_args, 
    train_dataset=train_data, 
    eval_dataset=val_data,
)

trainer.train()

bert2bert.save_pretrained('models/bert')

metric = load_metric("bleu")

import numpy as np

training_args.predict_with_generate = True

trainer = Seq2SeqTrainer(
    model=bert2bert,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
)

trainer.evaluate()

input_text = '<SOS> <horror> The American film industry is awash in controversy after the premiere of a movie starring Ria Mitala. The director Charles Martinis is the man behind the <EOS>'

input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors="pt").input_ids
input_ids.to(device=device)
out = bert2bert.generate(input_ids.to(device=device)).to(device=device)
print(tokenizer.decode(out[0]))

